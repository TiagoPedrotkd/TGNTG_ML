{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 36px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Model Optimization (MO) for Workers' Compensation Claims\n",
    "</h1>\n",
    "<hr style=\"border: 2px solid #4A90E2;\">\n",
    "\n",
    "<h2 style=\"text-align: center; color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 36px; text-shadow: 2px 2px #D1D1D1;\">Required Imports</h2>\n",
    "\n",
    "<hr style=\"border: 2px solid #4A90E2;\">\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">Package Descriptions</h3>\n",
    "<ul style=\"font-family: 'Arial', sans-serif;\">\n",
    "    <li><strong>pandas</strong>: For data manipulation and analysis, enabling easy reading and handling of dataframes.</li>\n",
    "    <li><strong>numpy</strong>: For efficient numerical operations and array manipulation.</li>\n",
    "    <li><strong>matplotlib.pyplot</strong>: To create data visualizations and plots.</li>\n",
    "    <li><strong>seaborn</strong>: For generating attractive and informative statistical visualizations.</li>\n",
    "    <li><strong>missingno</strong>: For visualizing and analyzing missing data, helping to better understand data quality.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import missingno as msng # type: ignore\n",
    "import sys # type: ignore\n",
    "import os # type: ignore\n",
    "\n",
    "from scipy import stats # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from meta_model_train import meta_model_rf, meta_model_et, meta_model_gb, meta_model_lr, meta_model_adaboost, meta_model_gnb, meta_model_cb\n",
    "from neural_network import neural_network\n",
    "from plots import plot_training_history, plot_confusion_matrix\n",
    "from predicitons_csv import save_predictions_to_csv\n",
    "from save_models import save_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center; color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 36px; text-shadow: 1px 1px #D1D1D1;\">\n",
    "    Data Loading\n",
    "</h2>\n",
    "<hr style=\"border: 1px solid #4A90E2;\">\n",
    "\n",
    "<p style=\"font-size: 18px; line-height: 1.6; font-family: 'Arial', sans-serif;\">\n",
    "    This section handles loading the dataset into the environment for further processing. Using <strong>pandas</strong>, we load the data into a structured dataframe, allowing for easy manipulation, exploration, and analysis throughout the project.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "\n",
    "X_train = pd.read_csv(path + \"X_train_post_FS.csv\")\n",
    "X_val = pd.read_csv(path + \"X_val_post_FS.csv\")\n",
    "\n",
    "y_train = pd.read_csv(path + \"y_train_post_FS.csv\")\n",
    "y_val = pd.read_csv(path + 'y_val_post_FS.csv')\n",
    "\n",
    "data_test = pd.read_csv(path + \"data_test_post_FS.csv\")\n",
    "\n",
    "data = pd.read_csv(path + \"Claim_Injury_Type_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = dict(zip(data['Encoded Value'], data['Claim Injury Type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center; color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 36px; text-shadow: 1px 1px #D1D1D1;\">\n",
    "    Data Scaling\n",
    "</h2>\n",
    "<hr style=\"border: 1px solid #4A90E2;\">\n",
    "\n",
    "<p style=\"font-size: 18px; line-height: 1.6; font-family: 'Arial', sans-serif;\">\n",
    "    In this section, we do data scaling is applied to normalize features, ensuring that all variables contribute equally to the model without being impacted by differing scales. This step is crucial for algorithms sensitive to feature magnitudes, such as regression and distance-based models.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 1px 1px #D1D1D1;\">\n",
    "    Data Scaling\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size: 18px; line-height: 1.6; font-family: 'Arial', sans-serif;\">\n",
    "    Data scaling is an essential preprocessing step to standardize the range of independent variables. By scaling features, we ensure that all variables contribute equally to model training, avoiding bias toward features with larger scales. Scaling is particularly important for algorithms sensitive to feature magnitudes, such as gradient-based methods and distance-based models.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the training data (this step calculates the mean and std dev of X_train only)\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation data using the same scaler (without fitting)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you also want to scale the test data in the same manner:\n",
    "X_test_scaled = scaler.transform(data_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center; color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 36px; text-shadow: 1px 1px #D1D1D1;\">\n",
    "    Model Selection\n",
    "</h2>\n",
    "<hr style=\"border: 1px solid #4A90E2;\">\n",
    "\n",
    "<p style=\"font-size: 18px; line-height: 1.6; font-family: 'Arial', sans-serif;\">\n",
    "    This section focuses on selecting the best-performing models for predicting workers' compensation claims outcomes. Various machine learning algorithms are evaluated based on their accuracy, interpretability, and suitability for the dataset, ensuring an optimal balance between predictive performance and computational efficiency.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote = SMOTE(random_state=42)\n",
    "#X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "X_train_resampled, y_train_resampled = X_train_scaled, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_model_run(model, X_train_resampled=None, y_train_resampled=None, data_test_FS=None, n_splits=5):\n",
    "    if X_train_resampled is None or y_train_resampled is None or data_test_FS is None:\n",
    "        raise ValueError(\"Os dados de treinamento e teste precisam ser fornecidos.\")\n",
    "    \n",
    "    if model == \"RandomForest\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_rf(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    elif model == \"GBoost\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_gb(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    elif model == \"ExtraTree\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_et(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    elif model == \"LogisticRegression\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_lr(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    elif model == \"AdaBoost\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_adaboost(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    elif model == \"GNB\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_gnb(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    elif model == \"CatBoost\":\n",
    "        f1_scores, validation_scores, oof_predictions, test_predictions = meta_model_cb(\n",
    "            X_train_resampled, y_train_resampled, data_test_FS, n_splits_n=n_splits\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Modelo inválido. Escolha entre 'RandomForest', 'GBoost', 'ExtraTree', 'LogisticRegression'.\")\n",
    "\n",
    "    return f1_scores, validation_scores, oof_predictions, test_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 32px; text-shadow: 2px 2px #D1D1D1;\">Claim Injury Type Prediction(Without Agreement Reached and WCB Decision)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Random Forest Classifier\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Random Forest Classifier</strong> is like a <strong>forest made up of many decision trees</strong>. Each tree looks at the problem and makes its own decision. For example, imagine we are trying to answer this question:\n",
    "</p>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    \"What is the right type of benefit for this case?\"\n",
    "</blockquote>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Each tree gives its own answer, but instead of relying on just one tree, the <strong>Random Forest</strong> gathers all the answers and chooses the one that <strong>most trees agree on</strong>. This makes the model <strong>more accurate and reliable</strong>, like a group of experts voting on the best solution.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Why Did We Choose the Random Forest?\n",
    "</h3>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Accurate and Reliable\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Random Forest works really well for <strong>complex problems</strong> and can handle <strong>multiple categories</strong>, just like in our project where we need to predict different types of benefits.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Avoids Errors (Overfitting)\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Instead of depending on one tree, it uses <strong>many trees</strong>, each looking at different parts of the data. This makes the predictions <strong>more stable</strong> and less likely to make mistakes with new data.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Handles Missing Data\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6; \">\n",
    "    Even if some information is missing, the model can still make <strong>good predictions</strong> without losing much accuracy.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Shows What’s Important\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Random Forest can tell us <strong>which variables are most important</strong> for making predictions. For example, it can show if <strong>age</strong>, <strong>type of accident</strong>, or the <strong>presence of an attorney</strong> matters most.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Fast and Scalable\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Even with a lot of data, Random Forest is <strong>quick to use</strong> and doesn’t require much effort to set up or improve.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Versatile\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The model works well for <strong>many types of problems</strong> and can discover <strong>complex relationships</strong> in the data, even when patterns are not straightforward.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Conclusion\n",
    "</h3>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Random Forest Classifier</strong> is a great choice for our project because:\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>It gives <strong>accurate and stable predictions</strong>.</li>\n",
    "    <li>It helps us understand <strong>which factors are most important</strong> in the decision-making process.</li>\n",
    "    <li>It avoids common problems like making errors on new data (<strong>overfitting</strong>).</li>\n",
    "    <li>It handles <strong>real-world challenges</strong> like missing or large amounts of data.</li>\n",
    "    <li>It’s <strong>easy to improve</strong> and adjust when needed.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    In short, <strong>Random Forest</strong> is a <strong>reliable</strong>, <strong>efficient</strong>, and <strong>easy-to-use tool</strong> that gives us <strong>clear and accurate results</strong> for predicting benefits in our project.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_rf, validation_scores_rf, oof_predictions_rf, test_predictions_rf = meta_model_run(\n",
    "    model=\"RandomForest\",\n",
    "    X_train_resampled=X_train_resampled,\n",
    "    y_train_resampled=y_train_resampled,\n",
    "    data_test_FS=data_test,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_rf, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(oof_predictions_rf, y_train_resampled)\n",
    "\n",
    "history = nn_model_rf.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_rf, \"RF_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The model demonstrates <strong>consistent learning</strong> over epochs, with both losses decreasing and stabilizing.\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>The <strong>accuracy</strong> metrics confirm that the model achieves good performance on unseen data.</li>\n",
    "    <li>There are no clear signs of <strong>overfitting</strong> or <strong>underfitting</strong>, suggesting that the model is well-tuned and generalizes effectively.</li>\n",
    "</ul>\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    This analysis confirms the model's stability and its capability to perform well on both training and validation datasets.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_rf, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The model demonstrates <strong>strong performance</strong> on the validation set, with high accuracy and balanced precision-recall across most classes.\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>Classes like <strong>6. PPD NSL</strong>, <strong>7. PTD</strong>, and <strong>8. DEATH</strong> achieve near-perfect <strong>precision</strong> and <strong>recall</strong>.</li>\n",
    "    <li>The <strong>classification report</strong> highlights areas of improvement, particularly for <strong>3. MED ONLY</strong>, which shows confusion with other similar classes.</li>\n",
    "</ul>\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    This evaluation confirms the model's overall effectiveness while identifying opportunities for further optimization in challenging classes.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_rf,\n",
    "    test_data=test_predictions_rf,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_RF_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_RF_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Extra Trees Classifier\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Extra Trees Classifier</strong> is an <strong>ensemble learning method</strong> that builds multiple decision trees to make predictions. Like Random Forest, it works by combining the outputs of several trees to improve accuracy and reduce errors. However, Extra Trees adds more randomness during the training process, which can further enhance generalization.\n",
    "</p>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    \"The model splits the data randomly at each decision point, making the Extra Trees Classifier robust and highly efficient.\"\n",
    "</blockquote>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    By averaging the results from multiple random trees, the <strong>Extra Trees Classifier</strong> delivers <strong>faster training</strong> and excellent results, especially on large datasets.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Why Did We Choose the Extra Trees Classifier?\n",
    "</h3>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Fast and Efficient\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Extra Trees uses <strong>random splits</strong> at decision nodes, which reduces the computational cost of building each tree. This makes it <strong>faster</strong> to train compared to other ensemble methods.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Reduces Overfitting\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The added randomness in splitting data prevents the model from <strong>overfitting</strong> to the training set, leading to <strong>better generalization</strong> on unseen data.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Handles Large Datasets\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Extra Trees performs exceptionally well on <strong>large datasets</strong> with many features. Its ability to handle high-dimensional data makes it an ideal choice for complex problems.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Robust to Noise\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Due to its random nature, the model is more <strong>robust to noise</strong> in the data, ensuring stable and consistent performance even with less clean datasets.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Provides Feature Importance\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Like Random Forest, Extra Trees can rank the <strong>importance of features</strong> in making predictions. This allows us to understand which variables contribute most to the results.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Conclusion\n",
    "</h3>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Extra Trees Classifier</strong> is an excellent choice for our project because:\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>It trains <strong>quickly</strong> and is <strong>computationally efficient</strong>.</li>\n",
    "    <li>It reduces <strong>overfitting</strong> and generalizes well to new data.</li>\n",
    "    <li>It handles <strong>large and noisy datasets</strong> with ease.</li>\n",
    "    <li>It ranks <strong>important features</strong> for better interpretability.</li>\n",
    "    <li>It is <strong>robust, scalable</strong>, and delivers <strong>consistent results</strong> for complex tasks.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    In short, the <strong>Extra Trees Classifier</strong> combines <strong>speed</strong>, <strong>robustness</strong>, and <strong>accuracy</strong>, making it a reliable solution for our predictive analysis.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_et, validation_scores_et, oof_predictions_et, test_predictions_et = meta_model_run(\n",
    "    model=\"ExtraTree\",\n",
    "    X_train_resampled=X_train_resampled,\n",
    "    y_train_resampled=y_train_resampled,\n",
    "    data_test_FS=data_test,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_et, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(oof_predictions_et, y_train_resampled)\n",
    "\n",
    "history = nn_model_et.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_et, \"ExtraTree_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The model demonstrates <strong>consistent learning</strong> over epochs, with both the <strong>training loss</strong> and <strong>validation loss</strong> decreasing and stabilizing.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>The <strong>Validation Loss</strong> remains consistently lower than the <strong>Train Loss</strong>, indicating good <strong>generalization</strong> of the model.</li>\n",
    "    <li>The <strong>Validation Accuracy</strong> is higher than the <strong>Train Accuracy</strong>, suggesting that the model is <strong>well-regularized</strong> and avoids overfitting.</li>\n",
    "    <li>Both accuracy and loss metrics stabilize towards the final epochs, confirming the model's <strong>training convergence</strong>.</li>\n",
    "</ul>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    This analysis highlights the model's stability and strong performance, with no evidence of overfitting or underfitting. The model generalizes effectively to unseen data.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_et, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The model demonstrates <strong>exceptional performance</strong> on the validation set, achieving a high overall accuracy of <strong>97%</strong>. Precision, recall, and F1-score are well-balanced across most classes.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>Classes like <strong>6. PPD NSL</strong>, <strong>7. PTD</strong>, and <strong>8. DEATH</strong> achieved near-perfect scores with <strong>precision</strong> and <strong>recall</strong> values of 1.00.</li>\n",
    "    <li>The class <strong>3. MED ONLY</strong> shows slightly lower performance, with a recall of <strong>0.89</strong>, indicating some confusion with other similar classes like <strong>4. TEMPORARY</strong>.</li>\n",
    "    <li>Overall, the high metrics in the <strong>classification report</strong> reflect a well-tuned and effective model.</li>\n",
    "</ul>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    This evaluation confirms the model's robustness, achieving <strong>excellent generalization</strong> while highlighting minimal areas for improvement in specific classes.\n",
    "</blockquote>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    The model achieves <strong>high accuracy</strong> and performs exceptionally well across all classes, with near-perfect scores for critical categories.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_et,\n",
    "    test_data=test_predictions_et,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_ExtraTree_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_ExtraTree_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Logistic Regression\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Logistic Regression</strong> model is a simple yet powerful statistical method that works well for **binary** and **multi-class classification problems**. It predicts the probability that a given input belongs to a particular class.\n",
    "</p>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    \"What is the probability that this observation belongs to a specific class?\"\n",
    "</blockquote>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Logistic Regression uses a <strong>mathematical function</strong> called the **sigmoid function** to convert input features into probabilities. It is widely used due to its simplicity, interpretability, and effectiveness.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Why Did We Choose Logistic Regression?\n",
    "</h3>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Simplicity and Interpretability\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Logistic Regression is easy to understand and interpret, as it provides clear probabilities for each class and allows us to analyze the impact of each feature on the predictions.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Efficient for Small to Medium Datasets\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Logistic Regression performs efficiently on **small to medium-sized datasets**, making it an excellent choice for problems where computational resources are limited.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Handles Linearly Separable Data\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    If the data is **linearly separable**, Logistic Regression can achieve very high performance without requiring complex tuning or transformations.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Probabilistic Predictions\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Logistic Regression outputs <strong>probabilities</strong> for each class, making it useful for tasks where confidence in predictions is important, such as risk analysis or medical diagnoses.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Regularization to Avoid Overfitting\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    Logistic Regression supports **regularization techniques** (e.g., L1 and L2) to prevent **overfitting** and improve generalization on unseen data.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Conclusion\n",
    "</h3>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Logistic Regression</strong> model is a strong choice for our project because:\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>It is <strong>simple to understand</strong> and highly interpretable.</li>\n",
    "    <li>It performs efficiently on <strong>small to medium-sized datasets</strong>.</li>\n",
    "    <li>It provides <strong>probabilistic outputs</strong> that help assess prediction confidence.</li>\n",
    "    <li>It can handle <strong>linearly separable data</strong> effectively.</li>\n",
    "    <li>It includes <strong>regularization techniques</strong> to avoid overfitting.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    In short, <strong>Logistic Regression</strong> is a <strong>reliable</strong>, <strong>interpretable</strong>, and <strong>efficient tool</strong> that works well for a variety of classification problems while maintaining simplicity.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_lr, validation_scores_lr, oof_predictions_lr, test_predictions_lr = meta_model_run(\n",
    "    model=\"LogisticRegression\",\n",
    "    X_train_resampled=X_train_resampled,\n",
    "    y_train_resampled=y_train_resampled,\n",
    "    data_test_FS=data_test,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_lr, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(oof_predictions_lr, y_train_resampled)\n",
    "\n",
    "history = nn_model_lr.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_lr, \"LR_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>consistent learning</strong> over the epochs, with both loss metrics gradually decreasing. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>The <strong>validation loss</strong> stabilizes over the final epochs, indicating that the model is adjusting well without significant oscillations.</li> <li>The <strong>accuracy</strong> for both training and validation improves progressively, suggesting the model generalizes effectively to unseen data.</li> <li>There are no clear signs of <strong>overfitting</strong> or <strong>underfitting</strong>, as the training and validation metrics remain closely aligned.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> This analysis highlights that the model is stable, well-tuned, and delivers consistent performance on both training and validation datasets. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_lr, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>strong performance</strong> on the validation set, with high accuracy across most classes, though there are areas for improvement in specific categories. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>Classes like <strong>7. PTD</strong>, <strong>8. DEATH</strong>, and <strong>6. PPD NSL</strong> achieve near-perfect precision and recall, with minimal misclassifications.</li> <li>However, there is noticeable <strong>confusion</strong> among the classes <strong>3. MED ONLY</strong>, <strong>4. TEMPORARY</strong>, and <strong>5. PPD SCH LOSS</strong>, as seen in their off-diagonal values.</li> <li>The class <strong>2. NON-COMP</strong> performs well overall, but a few misclassifications are evident, particularly with <strong>3. MED ONLY</strong>.</li> <li>Despite these challenges, the model’s performance remains robust, achieving solid classification results for the majority of the labels.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The analysis highlights the model’s <strong>strong generalization</strong>, with exceptional results for critical classes and opportunities for refinement in handling overlapping categories. </blockquote> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> With targeted fine-tuning, particularly for classes prone to confusion, the model could achieve even higher precision and recall across all categories. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_lr,\n",
    "    test_data=test_predictions_lr,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_LR_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_LR_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    AdaBoost Classifier\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>AdaBoost Classifier</strong> is like a team of weak learners working together to make better predictions. Instead of using a single strong model, AdaBoost combines multiple **simple models** (like decision stumps) and boosts their performance by focusing on the mistakes made in previous iterations.\n",
    "</p>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    \"How can we improve our predictions by combining multiple weak models?\"\n",
    "</blockquote>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    AdaBoost assigns more weight to **difficult examples** that were misclassified earlier. By repeatedly training new models and adjusting weights, it builds a final model that is **accurate and reliable**, despite starting with weak learners.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Why Did We Choose the AdaBoost Classifier?\n",
    "</h3>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Combines Simplicity and Power\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    AdaBoost uses **simple models** (like decision stumps) and combines them into a single, more powerful model. This makes it lightweight yet effective for many classification tasks.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Focuses on Mistakes\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    By giving more weight to **misclassified examples**, AdaBoost ensures that hard-to-predict data points are given extra attention in the next iteration, improving overall performance.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Robust and Accurate\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    AdaBoost is known for its **high accuracy** on classification tasks, especially when there is a balance between the classes. It works well even with noisy data.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It Avoids Overfitting\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    AdaBoost is less prone to **overfitting** because it uses an ensemble of simple models rather than a complex single model.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Versatile\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The AdaBoost algorithm can be applied to a wide range of classifiers, making it highly **versatile** and adaptable to different problems.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Conclusion\n",
    "</h3>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>AdaBoost Classifier</strong> is a strong choice for our project because:\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>It combines multiple **simple models** to achieve <strong>high accuracy</strong>.</li>\n",
    "    <li>It focuses on improving predictions for **difficult examples**.</li>\n",
    "    <li>It is **robust to noise** and less prone to <strong>overfitting</strong>.</li>\n",
    "    <li>It is **versatile** and can adapt to various types of classification problems.</li>\n",
    "    <li>It is lightweight and **easy to implement** while delivering powerful results.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    In short, <strong>AdaBoost</strong> is an <strong>efficient</strong> and <strong>powerful tool</strong> that enhances the performance of weak learners to deliver reliable and accurate results for our project.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_ada, validation_scores_ada, oof_predictions_ada, test_predictions_ada = meta_model_run(\n",
    "    model=\"AdaBoost\",\n",
    "    X_train_resampled=X_train_resampled,\n",
    "    y_train_resampled=y_train_resampled,\n",
    "    data_test_FS=data_test,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_ada, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(oof_predictions_ada, y_train_resampled)\n",
    "\n",
    "history = nn_model_ada.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_ada, \"ADA_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>consistent learning</strong> over the epochs, but with some variations in validation loss. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>The <strong>training loss</strong> decreases continuously, while the <strong>validation loss</strong> fluctuates slightly before stabilizing, suggesting that the model is still adapting.</li> <li>The <strong>validation accuracy</strong> surpasses the training accuracy, which may indicate a good fit for unseen data.</li> <li>Despite fluctuations in validation loss, there are no clear signs of <strong>overfitting</strong> or <strong>underfitting</strong>, as the metrics remain relatively close.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The model demonstrates promising performance, with potential fine-tuning needed to stabilize validation loss in future epochs. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_ada, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model exhibits <strong>strong performance</strong> on the validation set, achieving reliable classification across most classes while highlighting a few areas for improvement. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>Classes like <strong>8. DEATH</strong> and <strong>7. PTD</strong> perform exceptionally well, with minimal misclassifications, reflecting high precision and recall.</li> <li>Noticeable confusion occurs between <strong>3. MED ONLY</strong> and <strong>2. NON-COMP</strong>, as well as between <strong>4. TEMPORARY</strong> and <strong>5. PPD SCH LOSS</strong>. This indicates overlapping patterns or similar features in these categories.</li> <li>Class <strong>6. PPD NSL</strong> demonstrates solid results, but misclassifications with <strong>7. PTD</strong> suggest opportunities for further optimization.</li> <li>Overall, most classes achieve high performance, with the diagonal dominance in the confusion matrix confirming effective generalization.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The model delivers <strong>consistent accuracy</strong> and handles critical categories effectively, though targeted adjustments for overlapping classes can further enhance precision. </blockquote> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> Addressing class imbalances and refining feature separation between similar labels will help eliminate remaining misclassifications. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_ada,\n",
    "    test_data=test_predictions_ada,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_ADA_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_ADA_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Gaussian Naive Bayes (GNB) Classifier\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Gaussian Naive Bayes Classifier</strong> is a probabilistic model based on Bayes' Theorem. It assumes that the features are <strong>conditionally independent</strong> given the class label and that the data follows a Gaussian (normal) distribution.\n",
    "</p>\n",
    "\n",
    "<blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\">\n",
    "    \"How likely is this data point to belong to each class based on the observed features?\"\n",
    "</blockquote>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The classifier calculates probabilities for each class and selects the class with the <strong>highest probability</strong>. This simplicity makes the GNB Classifier a <strong>fast and efficient</strong> model for certain types of problems.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Why Did We Choose the Gaussian Naive Bayes Classifier?\n",
    "</h3>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    It’s Simple and Fast\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The GNB Classifier is computationally <strong>lightweight</strong> and performs well on smaller datasets. It is easy to implement and provides results very quickly.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Works Well with High-Dimensional Data\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    GNB performs well in problems where the number of features is large relative to the number of data points, such as text classification or spam detection.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Assumes Feature Independence\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    While real-world data may not fully satisfy this assumption, the GNB Classifier often performs surprisingly well even when features are correlated.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\">\n",
    "    Probabilistic Outputs\n",
    "</h4>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The GNB Classifier provides <strong>probabilities</strong> for each class, which can be useful for understanding uncertainty and making informed decisions.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">\n",
    "    Conclusion\n",
    "</h3>\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    The <strong>Gaussian Naive Bayes Classifier</strong> is a strong choice for our project because:\n",
    "</p>\n",
    "<ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    <li>It is <strong>simple</strong>, <strong>fast</strong>, and easy to implement.</li>\n",
    "    <li>It works well on <strong>high-dimensional data</strong>.</li>\n",
    "    <li>It provides <strong>probabilistic outputs</strong> for decision-making.</li>\n",
    "    <li>It performs surprisingly well, even when the independence assumption is not fully satisfied.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">\n",
    "    In short, the <strong>GNB Classifier</strong> is a <strong>lightweight</strong> and <strong>effective tool</strong> that delivers <strong>reliable</strong> and <strong>interpretable</strong> results for classification problems in our project.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_gnb, validation_scores_gnb, oof_predictions_gnb, test_predictions_gnb = meta_model_run(\n",
    "    model=\"GNB\",\n",
    "    X_train_resampled=X_train_resampled,\n",
    "    y_train_resampled=y_train_resampled,\n",
    "    data_test_FS=data_test,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_gnb, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(oof_predictions_gnb, y_train_resampled)\n",
    "\n",
    "history = nn_model_gnb.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_gnb, \"GNG_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>stable performance</strong> over the epochs, with small continuous improvements. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>Both <strong>training loss</strong> and <strong>validation loss</strong> show an overall reduction, with slight oscillations in the final epochs, indicating progress in the model's adjustment.</li> <li>The <strong>validation accuracy</strong> consistently exceeds training accuracy, suggesting that the model generalizes well to unseen data.</li> <li>The model does not exhibit signs of <strong>overfitting</strong> or <strong>underfitting</strong>, although the rate of improvement appears to be slowing down.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The analysis highlights solid performance, with room for fine-tuning to further improve convergence and maximize learning. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_gnb, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>solid performance</strong> on the validation set, with strong diagonal dominance in the confusion matrix, indicating effective classification for most classes. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>Classes like <strong>7. PTD</strong> and <strong>8. DEATH</strong> achieve exceptional results, showing very few misclassifications and high precision.</li> <li>There is some noticeable confusion between <strong>3. MED ONLY</strong> and <strong>2. NON-COMP</strong>, as well as between <strong>4. TEMPORARY</strong> and <strong>7. PTD</strong>, which may indicate overlapping features.</li> <li>The class <strong>5. PPD SCH LOSS</strong> also shows moderate misclassifications with classes <strong>6. PPD NSL</strong> and <strong>4. TEMPORARY</strong>, suggesting a need for better feature separation.</li> <li>Despite these challenges, the overall performance remains strong, with the majority of predictions concentrated along the correct diagonal. </li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The model delivers <strong>high accuracy</strong> across critical classes, while small adjustments to feature separation for overlapping labels can further enhance overall precision. </blockquote> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> With targeted improvements for classes prone to misclassification, the model's already strong generalization capabilities can be optimized further. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_gnb,\n",
    "    test_data=test_predictions_gnb,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_GNB_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_GNB_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\">Ensemble Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 28px; text-shadow: 2px 2px #D1D1D1;\"> Weighted Averaging Ensemble </h3> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The <strong>Weighted Averaging Ensemble</strong> is an ensemble technique that combines predictions from multiple models. Unlike simple averaging, it assigns a specific <strong>weight</strong> to each model based on its performance, ensuring that the best-performing models contribute more to the final prediction. </p> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> \"How can we leverage the strengths of multiple models to achieve higher accuracy?\" </blockquote> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The ensemble combines predictions by calculating the <strong>weighted average</strong> of individual model outputs. This strategy improves performance and reduces the risk of relying on a single model’s weaknesses. </p> <h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\"> Why Did We Choose the Weighted Averaging Ensemble? </h3> <h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\"> Combines Model Strengths </h4> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> By assigning higher weights to models with stronger performance, the ensemble leverages the <strong>strengths</strong> of the best models while minimizing the impact of weaker ones. </p> <h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\"> Reduces Overfitting Risk </h4> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> Combining multiple models reduces the likelihood of <strong>overfitting</strong> to the training data. The ensemble smoothens predictions and enhances generalization. </p> <h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\"> Improves Predictive Performance </h4> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> Weighted averaging typically improves accuracy by combining predictions in a way that compensates for individual model errors. </p> <h4 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 20px;\"> Flexibility with Model Selection </h4> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The ensemble allows the use of <strong>diverse models</strong> (e.g., decision trees, neural networks, or boosting models) and integrates their outputs effectively. </p> <h3 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\"> Conclusion </h3> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The <strong>Weighted Averaging Ensemble</strong> is a powerful choice for our project because: </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>It combines the <strong>strengths</strong> of multiple models.</li> <li>It reduces the <strong>risk of overfitting</strong>.</li> <li>It improves <strong>predictive accuracy</strong> and generalization.</li> <li>It provides <strong>flexibility</strong> to integrate various model types.</li> </ul> <p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> In summary, the <strong>Weighted Averaging Ensemble</strong> offers a robust, flexible, and effective approach to enhance model performance and deliver <strong>reliable</strong> predictions for our project. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo do weight's através do desempenho\n",
    "#Desempenho sem rede neuronal\n",
    "f1_rf=np.mean(f1_scores_rf)\n",
    "f1_et=np.mean(f1_scores_et)\n",
    "f1_ada=np.mean(f1_scores_ada)\n",
    "f1_gnb=np.mean(f1_scores_gnb)\n",
    "f1_lr=np.mean(f1_scores_lr)\n",
    "\n",
    "#Desempenho com rede neuronal\n",
    "f1_rf_nn=0.93\n",
    "f1_et_nn = 0.97\n",
    "f1_ada_nn = 0.66\n",
    "f1_gnb_nn = 0.52 \n",
    "f1_lr_nn = 0.68\n",
    "\n",
    "weights = [(((f1_rf) + (f1_rf_nn)) / 2), (((f1_et) + (f1_et_nn)) / 2), (((f1_ada) + (f1_ada_nn)) / 2), (((f1_gnb) + (f1_gnb_nn)) / 2), (((f1_lr) + (f1_lr_nn)) / 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_test_predictions = (\n",
    "    weights[0] * test_predictions_rf +\n",
    "    weights[1] * test_predictions_et +\n",
    "    weights[2] * test_predictions_ada +\n",
    "    weights[3] * test_predictions_gnb +\n",
    "    weights[4] * test_predictions_lr\n",
    ")\n",
    "\n",
    "ensemble_oof_predictions = (\n",
    "    weights[0] * oof_predictions_rf +\n",
    "    weights[1] * oof_predictions_et +\n",
    "    weights[2] * oof_predictions_ada +\n",
    "    weights[3] * oof_predictions_gnb +\n",
    "    weights[4] * oof_predictions_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_predictions = ensemble_test_predictions\n",
    "final_oof_predictions = ensemble_oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_weight_ensemble, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(final_oof_predictions, y_train_resampled)\n",
    "\n",
    "history = nn_model_weight_ensemble.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_weight_ensemble, \"WeightedEnsemble_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>stable performance</strong> over the epochs, with continuous improvements in both loss and accuracy metrics. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>The <strong>training loss</strong> decreases consistently, while the <strong>validation loss</strong> stabilizes at low levels, indicating efficient model convergence.</li> <li>The <strong>validation accuracy</strong> remains consistently higher than the training accuracy, suggesting that the model generalizes very well to unseen data.</li> <li>The model does not show signs of <strong>overfitting</strong> or <strong>underfitting</strong>, as the loss and accuracy curves maintain a balanced trend.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The analysis reveals that the model is <strong>well-tuned</strong>, achieving solid and consistent performance, with minimal opportunities for further refinement. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_weight_ensemble, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>exceptional performance</strong> on the validation set, achieving an impressive overall accuracy with highly reliable predictions across most classes. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>Classes like <strong>6. PPD NSL</strong>, <strong>7. PTD</strong>, and <strong>8. DEATH</strong> achieved near-perfect scores with <strong>precision</strong> and <strong>recall</strong> values close to 1.00.</li> <li>The class <strong>4. TEMPORARY</strong> shows slight confusion with classes like <strong>3. MED ONLY</strong> and <strong>5. PPD SCH LOSS</strong>, as seen in the off-diagonal values, but still maintains a high overall recall.</li> <li>Class <strong>3. MED ONLY</strong> exhibits minor misclassifications, suggesting overlapping features with <strong>4. TEMPORARY</strong>, yet it maintains strong predictive performance.</li> <li>Overall, the confusion matrix highlights the model's ability to generalize well across the majority of classes, with very few significant misclassifications.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> This evaluation confirms the model's <strong>robustness</strong> and <strong>reliability</strong>, achieving excellent results across critical classes while demonstrating minor areas for improvement. </blockquote> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The model delivers <strong>high accuracy</strong> and performs exceptionally well, maintaining near-perfect scores in key categories and ensuring strong generalization on unseen data. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_weight_ensemble,\n",
    "    test_data=final_test_predictions,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_Ensemble_Weight_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_Ensemble_Weight_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">Stacking Ensemble</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_train = np.column_stack((oof_predictions_rf, oof_predictions_et, oof_predictions_ada, oof_predictions_gnb, oof_predictions_lr))\n",
    "X_meta_test = np.column_stack((test_predictions_rf, test_predictions_et, test_predictions_ada, test_predictions_gnb, test_predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_stacking_ensemble, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(X_meta_train, y_train_resampled)\n",
    "\n",
    "history = nn_model_stacking_ensemble.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_stacking_ensemble, \"StackingEnsemble_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates a <strong>stable performance</strong> over the epochs, with gradual improvements in both loss and accuracy metrics. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>The <strong>training loss</strong> and <strong>validation loss</strong> exhibit a steady overall reduction, with minor oscillations towards the final epochs, indicating a smooth and consistent learning process.</li> <li>The <strong>validation accuracy</strong> remains consistently higher than the training accuracy, suggesting that the model generalizes well to unseen data.</li> <li>There are no clear signs of <strong>overfitting</strong> or <strong>underfitting</strong>, as both loss and accuracy curves maintain a close alignment throughout the training process.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The analysis highlights a well-tuned and reliable model, achieving strong generalization with minimal opportunities for further improvement. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_stacking_ensemble, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> The model demonstrates <strong>exceptional performance</strong> on the validation set, achieving a high overall accuracy of <strong>97%</strong>. Precision, recall, and F1-score are well-balanced across nearly all classes. </p> <ul style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\"> <li>Classes such as <strong>6. PPD NSL</strong>, <strong>7. PTD</strong>, and <strong>8. DEATH</strong> achieved <strong>near-perfect scores</strong> with precision and recall values of 1.00, reflecting their strong predictive performance.</li> <li>The class <strong>3. MED ONLY</strong> shows slightly lower performance, with some confusion observed with class <strong>4. TEMPORARY</strong>, as indicated by its off-diagonal values.</li> <li>Minimal misclassifications are present overall, suggesting that the model is well-calibrated and effectively generalizes to unseen data.</li> </ul> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> This evaluation highlights the model's <strong>robustness</strong> and its ability to achieve consistent, high accuracy across critical classes, with only minor areas for improvement. </blockquote> <blockquote style=\"font-style:italic; border-left:4px solid #4A90E2; padding-left:10px; margin-left: 0;\"> The model delivers <strong>outstanding performance</strong>, with near-perfect results in key categories and minimal confusion, making it highly reliable for real-world applications. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_stacking_ensemble,\n",
    "    test_data=X_meta_test,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_Ensemble_Stacking_NN_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_Ensemble_Stacking_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color: #4A90E2; font-family: 'Arial', sans-serif; font-size: 24px; text-shadow: 2px 2px #D1D1D1;\">Models Extra</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.6;\">We ran the CatBoost model together with the Neural Network (NN) model, which gave us the highest score. However, due to errors with TensorFlow, the computer processors, and the BIOS, we were unable to run the model again. As a result of these errors, we lost the best-performing model. But below we give the code for the catboost running</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_cb, validation_scores_cb, oof_predictions_cb, test_predictions_cb = meta_model_run(\n",
    "    model=\"CatBoost\",\n",
    "    X_train_resampled=X_train_resampled,\n",
    "    y_train_resampled=y_train_resampled,\n",
    "    data_test_FS=data_test,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_cb, X_nn_train, X_nn_val, y_nn_val, y_nn_train, early_stopping, reduce_lr = neural_network(oof_predictions_cb, y_train_resampled)\n",
    "\n",
    "history = nn_model_cb.fit(\n",
    "    X_nn_train, y_nn_train,\n",
    "    validation_data=(X_nn_val, y_nn_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "save_model(nn_model_cb, \"CatBoost_NN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model=nn_model_cb, X_val=X_nn_val, y_val=y_nn_val,\n",
    "    class_mapping=mapping_dict, title=\"Matriz de Confusão - Conjunto de Validação\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(\n",
    "    model=nn_model_cb,\n",
    "    test_data=test_predictions_cb,\n",
    "    claim_ids=data_test[\"Claim Identifier\"],\n",
    "    class_mapping=mapping_dict,\n",
    "    output_path=\"../predictions/group_40_KFold_CatBoost_NN_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons_data = pd.read_csv('../predictions/group_40_KFold_CatBoost_NN_predictions.csv')\n",
    "values = predicitons_data['Claim Injury Type'].value_counts()\n",
    "values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
